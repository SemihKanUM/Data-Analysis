{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "# Web Scraping\n",
    "\n",
    "In this notebook we will explore how we can extract data from a web-page using automatic scraping and crawling with [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n",
    "\n",
    "First, we'll talk a bit about HTML though. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. HTML and the DOM\n",
    "\n",
    "We will scrape web-pages that are (partially) written in HTML and represented in the DOM. DOM stands for  Document Object Model, while HTML stands for “HyperText Markup Language”. 25 years ago, that used to be a meaningful description of what HTML actually did: it has links (hypertext), and it is a markup language. The latest version of HTML, however, the HTML5 standard, does much, much more: graphics, audio, video, etc. So it is easier to think of HTML as “whatever it is that web browsers know how to interpret”, and just not think about the actual term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 HTML Elements\n",
    "\n",
    "The important thing about HTML is that the markup is represented by elements. An HTML element is a portion of the content that is surrounded by a pair of tags of the same name. Like this:\n",
    "\n",
    "```html\n",
    "<strong>This is an HTML element.</strong>\n",
    "```\n",
    "\n",
    "In this element, strong is the name of the tag; the open tag is `<strong>`, and the matching closing tag is `</strong>`. The way you should interpret this is that the text “This is an HTML element” should be “strong”, i.e., typically this will be bold text.\n",
    "\n",
    "HTML elements can and commonly do nest:\n",
    "\n",
    "```html\n",
    "<strong>This is strong, and <u>this is underlined and strong.</u></strong>\n",
    "```\n",
    "\n",
    "In addition to the names, opening tags can contain extra information about the element. These are called attributes:\n",
    "\n",
    "```html\n",
    "<a href='http://www.google.com'>A link to Google's main page</a>\n",
    "```\n",
    "\n",
    "In this case, we’re using the `a` element which stood for “anchor”, but now is almost universally used as a “link”. The attribute `href` means “HTML reference”, which actually makes sense for a change. The meaning given to each attribute changes from element to element.\n",
    "\n",
    "Important attributes for our purposes are `id` and `class`. The id attribute gives the attribute a unique identifier, which can then be used to access the element programmatically. Think of it as making the element accessible via a global variable.  \n",
    "\n",
    "The class is similar but is intendent to be applied to a whole “class” of elements. \n",
    "\n",
    "HTML pages require some boilerplate. Here is a minimal page: \n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title></title>\n",
    "</head>\n",
    "<body>\n",
    "Hello World! What's up?\n",
    "</body>\n",
    "</html>\n",
    "``` \n",
    "\n",
    "The `<head>` contains meta-information such as the titel of the site, the `<body>` contains the actual data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 HTML Hierarchy\n",
    "\n",
    "Data in HTML is often structured hierarchically: \n",
    "\n",
    "```html\n",
    "<body>\n",
    "  <article>\n",
    "    <span class=\"date\">Published: 1969-10-22</span>\n",
    "    <span class=\"author\">Led Zeppelin</span>\n",
    "    <h1>Ramble On</h1>\n",
    "    <div class=\"content\">\n",
    "    Leaves are falling all around, It's time I was on my way. \n",
    "    Thanks to you, I'm much obliged for such a pleasant stay. \n",
    "    But now it's time for me to go. The autumn moon lights my way. \n",
    "    For now I smell the rain, and with it pain, and it's headed my way. \n",
    "    </div>\n",
    "  </article>\n",
    "  <article>\n",
    "    <span class=\"date\">Published: 2016-05-03</span>\n",
    "    <span class=\"author\">Radiohead</span>\n",
    "    <h1>Burn the Witch</h1>\n",
    "    <div class=\"content\">\n",
    "    Stay in the shadows\n",
    "    Cheer at the gallows\n",
    "    This is a round up\n",
    "    This is a low flying panic attack\n",
    "    Sing a song on the jukebox that goes\n",
    "    Burn the witch\n",
    "    Burn the witch\n",
    "    We know where you live\n",
    "    </div>\n",
    "  </article>\n",
    "</body>\n",
    "```\n",
    "\n",
    "Here, the title of the song is nested three levels deep: `body > article > h1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 HTML Tables\n",
    "\n",
    "Data is also often stored in HTML tables, which are enclosed in a `<table>` tag. `<tr>` indicates a row (table row), `<th>` and `<td>` are used to demark cells, either header cells (`<th>`, table header) or regular cells (`<td>`, table data). Here's an example: \n",
    "\n",
    "```html\n",
    "<table>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th>The Beatles</th>\n",
    "        <th>Led Zeppelin</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td># Band Members</td>\n",
    "        <td>4</td>\n",
    "        <td>4</td>\n",
    "    </tr>\n",
    "</table>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 The DOM\n",
    "\n",
    "As we have seen above, a markup document looks a lot like a tree: it has a root, the HTML element, and elements can have children that are containing elements themselves.\n",
    "\n",
    "While HTML is a textual representation of a markup document, the DOM is a programming interface for it. Also the DOM represents the state of a page as it's rendered, that (nowadays) doesn't mean that there is an underlying HTML document that corresponds to that exactly. Rather, the DOM is dynamically generated with, e.g., JavaScript. \n",
    "\n",
    "In this class we will use “DOM” to mean the tree created by the web browsers to represent the document.\n",
    "\n",
    "#### Inspecting the DOM in a browser\n",
    "\n",
    "Perhaps the most important habit when scraping is to investigate the source of a page using the Developer Tools. In this case, we’ll look at the **element tree**, by clicking on the menu bar: View → Developer → Developer Tools.\n",
    "\n",
    "Alternatively, you can right click on any part of the webpage, and choose “Inspect Element”. Notice that there can be a big difference between what is in the DOM and what is in the source.\n",
    "\n",
    "Take a look at the DOM of [this html page](lyrics.html). Next, we'll scrape the data from this page! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Scraping with BeautifulSoup\n",
    "\n",
    "[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) is a Python library design for computationally extracting data from html documents. It supports navigating in the DOM and retreiving exactly the data elements you need.\n",
    "\n",
    "Let's start with a simple example using the [lyrics.html](lyrics.html) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<!DOCTYPE html>\n",
       "\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "<meta charset=\"utf-8\"/>\n",
       "<title>Lyrics</title>\n",
       "</head>\n",
       "<body>\n",
       "<article id=\"zep\">\n",
       "<span class=\"date\">Published: 1969-10-22</span>\n",
       "<span class=\"author\">Led Zeppelin</span>\n",
       "<h1>Ramble On</h1>\n",
       "<div class=\"content\">\n",
       "    Leaves are falling all around, It's time I was on my way.\n",
       "    Thanks to you, I'm much obliged for such a pleasant stay.\n",
       "    But now it's time for me to go. The autumn moon lights my way.\n",
       "    For now I smell the rain, and with it pain, and it's headed my way.\n",
       "    </div>\n",
       "</article>\n",
       "<article id=\"radio\">\n",
       "<span class=\"date\">Published: 2016-05-03</span>\n",
       "<span class=\"author\">Radiohead</span>\n",
       "<h1>Burn the Witch</h1>\n",
       "<div class=\"content\">\n",
       "    Stay in the shadows\n",
       "    Cheer at the gallows\n",
       "    This is a round up\n",
       "    This is a low flying panic attack\n",
       "    Sing a song on the jukebox that goes\n",
       "    Burn the witch\n",
       "    Burn the witch\n",
       "    We know where you live\n",
       "    </div>\n",
       "</article>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# we tell BeautifulSoup and tell it which parser to use\n",
    "song_soup = BeautifulSoup(open(\"lyrics.html\"), \"html.parser\")\n",
    "# the output corresponds exactly to the html file\n",
    "song_soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, generated (minified) html can be hard to read (not in this case), so we can format it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <title>\n",
      "   Lyrics\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <article id=\"zep\">\n",
      "   <span class=\"date\">\n",
      "    Published: 1969-10-22\n",
      "   </span>\n",
      "   <span class=\"author\">\n",
      "    Led Zeppelin\n",
      "   </span>\n",
      "   <h1>\n",
      "    Ramble On\n",
      "   </h1>\n",
      "   <div class=\"content\">\n",
      "    Leaves are falling all around, It's time I was on my way.\n",
      "    Thanks to you, I'm much obliged for such a pleasant stay.\n",
      "    But now it's time for me to go. The autumn moon lights my way.\n",
      "    For now I smell the rain, and with it pain, and it's headed my way.\n",
      "   </div>\n",
      "  </article>\n",
      "  <article id=\"radio\">\n",
      "   <span class=\"date\">\n",
      "    Published: 2016-05-03\n",
      "   </span>\n",
      "   <span class=\"author\">\n",
      "    Radiohead\n",
      "   </span>\n",
      "   <h1>\n",
      "    Burn the Witch\n",
      "   </h1>\n",
      "   <div class=\"content\">\n",
      "    Stay in the shadows\n",
      "    Cheer at the gallows\n",
      "    This is a round up\n",
      "    This is a low flying panic attack\n",
      "    Sing a song on the jukebox that goes\n",
      "    Burn the witch\n",
      "    Burn the witch\n",
      "    We know where you live\n",
      "   </div>\n",
      "  </article>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(song_soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Navigating parsed DOM\n",
    "Assuming that BeautifulSoup correctly parsed a DOM out of lyrics.html, we can access content by tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<title>Lyrics</title>\n"
     ]
    }
   ],
   "source": [
    "# get the title tag\n",
    "print(type(song_soup))\n",
    "print(type(song_soup.title))\n",
    "print(song_soup.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And get the text out of the tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lyrics'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_soup.title.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directly accessing an element works for the first occurence of a tag, but we don't get the others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"content\">\n",
       "    Leaves are falling all around, It's time I was on my way.\n",
       "    Thanks to you, I'm much obliged for such a pleasant stay.\n",
       "    But now it's time for me to go. The autumn moon lights my way.\n",
       "    For now I smell the rain, and with it pain, and it's headed my way.\n",
       "    </div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_soup.div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can retrieve the text content of an element: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Leaves are falling all around, It's time I was on my way.\n",
      "    Thanks to you, I'm much obliged for such a pleasant stay.\n",
      "    But now it's time for me to go. The autumn moon lights my way.\n",
      "    For now I smell the rain, and with it pain, and it's headed my way.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(song_soup.div.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use attributes to find a specific element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<article id=\"zep\">\n",
       "<span class=\"date\">Published: 1969-10-22</span>\n",
       "<span class=\"author\">Led Zeppelin</span>\n",
       "<h1>Ramble On</h1>\n",
       "<div class=\"content\">\n",
       "    Leaves are falling all around, It's time I was on my way.\n",
       "    Thanks to you, I'm much obliged for such a pleasant stay.\n",
       "    But now it's time for me to go. The autumn moon lights my way.\n",
       "    For now I smell the rain, and with it pain, and it's headed my way.\n",
       "    </div>\n",
       "</article>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_soup.find(id=\"zep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can also get only the text, not the html markup with [`find()`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Published: 1969-10-22\n",
      "Led Zeppelin\n",
      "Ramble On\n",
      "\n",
      "    Leaves are falling all around, It's time I was on my way.\n",
      "    Thanks to you, I'm much obliged for such a pleasant stay.\n",
      "    But now it's time for me to go. The autumn moon lights my way.\n",
      "    For now I smell the rain, and with it pain, and it's headed my way.\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = song_soup.find(id=\"zep\").get_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `[find_all()](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#calling-a-tag-is-like-calling-find-all)` to get all instances of a tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h1>Ramble On</h1>, <h1>Burn the Witch</h1>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1s = song_soup.find_all(\"h1\")\n",
    "h1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a list of beautiful soup elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(h1s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to get the text out of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ramble On', 'Burn the Witch']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_h1s = [tag.get_text() for tag in h1s]\n",
    "string_h1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `find_all` is so commonly used, you can use a shortcut by just calling directly on an object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"content\">\n",
       "     Leaves are falling all around, It's time I was on my way.\n",
       "     Thanks to you, I'm much obliged for such a pleasant stay.\n",
       "     But now it's time for me to go. The autumn moon lights my way.\n",
       "     For now I smell the rain, and with it pain, and it's headed my way.\n",
       "     </div>,\n",
       " <div class=\"content\">\n",
       "     Stay in the shadows\n",
       "     Cheer at the gallows\n",
       "     This is a round up\n",
       "     This is a low flying panic attack\n",
       "     Sing a song on the jukebox that goes\n",
       "     Burn the witch\n",
       "     Burn the witch\n",
       "     We know where you live\n",
       "     </div>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_soup(\"div\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can address the elements in the returned object directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"content\">\n",
       "    Stay in the shadows\n",
       "    Cheer at the gallows\n",
       "    This is a round up\n",
       "    This is a low flying panic attack\n",
       "    Sing a song on the jukebox that goes\n",
       "    Burn the witch\n",
       "    Burn the witch\n",
       "    We know where you live\n",
       "    </div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_soup(\"div\")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or iterate over it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "\n",
      "    Leaves are falling all around, It's time I was on my way.\n",
      "    Thanks to you, I'm much obliged for such a pleasant stay.\n",
      "    But now it's time for me to go. The autumn moon lights my way.\n",
      "    For now I smell the rain, and with it pain, and it's headed my way.\n",
      "    \n",
      "---\n",
      "\n",
      "    Stay in the shadows\n",
      "    Cheer at the gallows\n",
      "    This is a round up\n",
      "    This is a low flying panic attack\n",
      "    Sing a song on the jukebox that goes\n",
      "    Burn the witch\n",
      "    Burn the witch\n",
      "    We know where you live\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "for p in song_soup.find_all(\"div\"):\n",
    "    print(\"---\")\n",
    "    print(p.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Using CSS Selectors\n",
    "\n",
    "CSS selectors offer an alternative way of accessing content. CSS Selectors apply, among others, to elements, classes, and IDs.\n",
    "\n",
    "Below is an example of how CSS is used to style different elements. \n",
    "\n",
    "\n",
    "```CSS\n",
    "/* Element Selector */\n",
    "article {\n",
    "  color: FireBrick;\n",
    "}\n",
    "\n",
    "/* ID selector */\n",
    "#myID {\n",
    "  color: Tomato;\n",
    "}\n",
    "\n",
    "/* Class selector */\n",
    ".myClass {\n",
    "  color: Aquamarine;\n",
    "}\n",
    "\n",
    "/* Child selector. Only DIRECT children match */\n",
    "p > b {\n",
    "  color: SteelBlue;\n",
    "}\n",
    "\n",
    "/* Descendant selector. Every time a b is nested within a div this matches */\n",
    "div b {\n",
    "  color: green;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "[Here is an example](https://jsfiddle.net/gxhqv26m/1/) with all the important selectors.\n",
    "\n",
    "Let's try this in Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"content\">\n",
       "     Leaves are falling all around, It's time I was on my way.\n",
       "     Thanks to you, I'm much obliged for such a pleasant stay.\n",
       "     But now it's time for me to go. The autumn moon lights my way.\n",
       "     For now I smell the rain, and with it pain, and it's headed my way.\n",
       "     </div>,\n",
       " <div class=\"content\">\n",
       "     Stay in the shadows\n",
       "     Cheer at the gallows\n",
       "     This is a round up\n",
       "     This is a low flying panic attack\n",
       "     Sing a song on the jukebox that goes\n",
       "     Burn the witch\n",
       "     Burn the witch\n",
       "     We know where you live\n",
       "     </div>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting all elements of class .content\n",
    "song_soup.select(\".content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"content\">\n",
       "     Stay in the shadows\n",
       "     Cheer at the gallows\n",
       "     This is a round up\n",
       "     This is a low flying panic attack\n",
       "     Sing a song on the jukebox that goes\n",
       "     Burn the witch\n",
       "     Burn the witch\n",
       "     We know where you live\n",
       "     </div>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting all divs that are somewhere below the id radio in the tree\n",
    "song_soup.select(\"#radio div\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now work out some complete examples of extracting information from a website. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Fetching a Website\n",
    "\n",
    "Downloading websites is easy and very efficient. It turns out, that you can cause quite a high load on a server when you scrape it. To avoid that, webmasters usually publish what kinds of scraping they allow on their websites. You should check out a website's terms of service and the `robots.txt` of a domain before crawling excessively. Terms of service are usually broad, so searching for “scraping” or “crawling” is a good idea.\n",
    "\n",
    "Let's take a look at [Google Scholar's robots.txt](https://scholar.google.com/robots.txt):\n",
    "\n",
    "```\n",
    "User-agent: *\n",
    "Disallow: /search\n",
    "Disallow: /index.html\n",
    "Disallow: /scholar\n",
    "Disallow: /citations?\n",
    "Allow: /citations?user=\n",
    "Disallow: /citations?*cstart=\n",
    "Disallow: /citations?user=*%40\n",
    "Disallow: /citations?user=*@\n",
    "Allow: /citations?view_op=list_classic_articles\n",
    "Allow: /citations?view_op=metrics_intro\n",
    "Allow: /citations?view_op=new_profile\n",
    "Allow: /citations?view_op=sitemap\n",
    "Allow: /citations?view_op=top_venues\n",
    "...\n",
    "```\n",
    "\n",
    "Here it specifies that you're not allowed to crawl a lot of the pages. The `/scholar` subdirectory is especially painful because it prohibits you from generating queries dynamically. \n",
    "\n",
    "It's also common that sites ask you to delay crawiling: \n",
    "\n",
    "```\n",
    "Crawl-delay: 30 \n",
    "Request-rate: 1/30 \n",
    "```\n",
    "\n",
    "You should respect those restrictions. Now, no one can stop you from running a request through a crawler, but sites like google scholar will block you VERY quickly if you request to many pages in a short time-frame.\n",
    "\n",
    "An alternative strategy to dynamically accessing the site you're crawling (as we're doing in the next example) is to download a local copy of the website and crawl that. This ensures that you hit the site only once per page. A good tool to achieve that is [wget](https://www.gnu.org/software/wget/). \n",
    "\n",
    "Here are some other interesting robots.txt files:\n",
    "\n",
    "https://www.youtube.com/robots.txt => looks like there was a robopocalypse in the 90'ties\n",
    "\n",
    "https://cordis.europa.eu/robots.txt => it's ok to get records but not in an .xml format... really? So instead of serving me raw .xml through an API you prefer me to hit your server with HTML generation requests? O_o Sounds like more work for both of us.\n",
    "\n",
    "https://www.thegazette.co.uk/robots.txt => somebody clearly did not want us to read the whole thing, it's damn long... but, it's interesting that they blacklist records of specific companies. So it's OK to scrape some companies but not some others? I wonder where I would be looking for my new story if I was an investigative journalist in the UK..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example: Exceptional Olympians\n",
    "\n",
    "Scrape data from [this wikipedia site](https://en.wikipedia.org/wiki/List_of_multiple_Olympic_medalists) about exceptional Olympic medalists. \n",
    "\n",
    "1. Download the html using urllib. \n",
    "2. Parse this html with BeautifulSoup.\n",
    "3. Extract the html that corresponds to the big table from the soup.\n",
    "4. Parse the table into a pandas dataframe. Hint: both the \"No.\" and the \"Total.\" column use row-spans which are tricky to parse, both with a pandas reader and manually. For the purpose of this exercise, exclude all rows that are not easy to parse (the first one is Bjørn Dæhlie).\n",
    "5. Create a table that shows for each country how many gold, silver, bronze, and total medals it won in that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the html using urllib. \n",
    "Parse this html with BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_multiple_Olympic_medalists\"\n",
    "\n",
    "req = urllib.request.Request(url)\n",
    "with urllib.request.urlopen(req) as response:\n",
    "    html = response.read()\n",
    "\n",
    "print(html)\n",
    "\n",
    "class_soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the html that corresponds to the big table from the soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can retrieve all tables, our desired table is the first one:\n",
    "table_html = class_soup(\"table\")[0]\n",
    "table_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the table into a pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes = pd.read_html(str(table_html), header=0)[0]\n",
    "athletes.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For cases where the row is screwed up, the bronze column is NaN, which we can use to filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes = athletes[pd.notnull(athletes[\"Bronze\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset to the relevant columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes = athletes[[\"Nation\", \"Gold\", \"Silver\", \"Bronze\"]]\n",
    "athletes.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping, summing, calculating the total, and sorting. If the .groupby() function still confuses you as much as it has confused me in the beginning, here's a great explanation: https://towardsdatascience.com/pandas-groupby-aggregate-transform-filter-c95ba3444bbb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = athletes.groupby(\"Nation\").sum()\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "countries[\"Total\"] = countries[\"Gold\"] + countries[\"Silver\"] + countries[\"Bronze\"]\n",
    "countries = countries.sort_values(\"Total\", ascending=False)\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's pick the top 5 countries and ditch the \"Total\" column to visualize amount of each medals\n",
    "df = countries.head(5).drop(columns=[\"Total\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "     \n",
    "barWidth = 0.25\n",
    "base = 10\n",
    "colors = ['#fff969','#b3b3b3','#7d5d30' ]\n",
    " \n",
    "# Make the plot\n",
    "for i, country in enumerate(df.index):\n",
    "    for j, medal in enumerate(df.columns):\n",
    "        plt.bar(i+0.25*j, df.loc[country, medal], color=colors[j], width=barWidth, edgecolor='white')\n",
    "        \n",
    "plt.xlabel('country', fontweight='bold')\n",
    "plt.ylabel('#medals', fontweight='bold')\n",
    "plt.xticks([r + barWidth for r in range(len(df.index))], df.index)\n",
    " \n",
    "# Create legend & Show graphic\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scraping Wrap-Up & Practical Considerations\n",
    "\n",
    "Scraping is a way to get information from website that were not designed to make data accessible. As such, it can often be **brittle**: a website change will break your scraping script. It is also often not welcome, as a scaper can cause a lot of traffic. \n",
    "\n",
    "Finally, many services make their data available through a well-defined interface, an API. Using an API is always a better idea than scraping, but scraping is a good fallback!\n",
    "\n",
    "The way we scraped information here also made the **assumption that HTML is generated consistently** based just on the URL. That is, unfortunately, less and less common, as websites adapt to browser types, resolutions, locales, but also as a lot of content is loaded dynamically e.g., via web-sockets. For example, many websites now auotmatically load more data once you scroll to the bottom of the page. Everything is great when the DOM is easy and loads into BeautifulSoup without issues. In practice, that is often not the case. Either DOM loading just fails to recognize some elements or the object tree has an obscurely complicated and/or dynamic structure. Or maybe you didn't even get the DOM that you actually wanted, because the request that you sent did not return the same HTML as you would see as a human!\n",
    "\n",
    "These websites couldn't be scraped with our approach, instead, a browser-emulation approach, using e.g., [Selenium](https://www.selenium.dev/) would be necessary. Today there is pretty much no alternative to the Selenium browser. It can run the Chromium driver, same as Google Chrome, Microsoft Edge and Brave, or the Gecko driver, same as Firefox. [Here is a recent tutorial](https://www.browserstack.com/guide/web-scraping-using-selenium-python) on how to set it up. This gives you access to all powerful ways of navigating web content: https://selenium-python.readthedocs.io/locating-elements.html. And can even run in a \"headless\" mode (i.e. without a GUI) so that you can easily scrape in the background or on a machine without a GUI. \n",
    "\n",
    "A noteworthy way of accessing elements that I want to mention is XPath, which is especially handy for manipulating XML format. It looks something like this:\n",
    "\n",
    "    xpath=\"//node_name[@node_attribute='node_value']\"\n",
    "    \n",
    "Guru99 offers a great overview: https://www.guru99.com/xpath-selenium.html\n",
    "\n",
    "If you ever MUST use a GUI (for example, to scrape not a website but rather some oldschool OS application), check out some interface automation tools. Business people call this Robotic Process Automation (RPA), but it's nothing more than automatically operating your mouse and keyboard: https://pyautogui.readthedocs.io/en/latest/. You can even combine it with some computer vision (OpenCV, Tesseract) for really cool results!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
